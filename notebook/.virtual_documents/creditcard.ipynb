import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


filename="creditcard.csv"


df = pd.read_csv(filename)


df.head()


df.info()


# Clean up column names
df.columns=df.columns.str.strip()
df.columns


df.describe(include='all')


#!pip install sweetviz


df.nunique()


import sweetviz as sv
report = sv.analyze(df)
report.show_notebook()



from scipy import stats
num= df.select_dtypes(include=np.number)
for i in num:
    print(i)
    print(stats.describe(df[i]))
    print(stats.shapiro(df[i]))


cat= df.astype('category')
cat.columns


data=df['class'].value_counts()
data


data=df['class'].value_counts()
sns.barplot(data=data)


df_age = df['age'].values.tolist()
sns.histplot(df_age)


df['purpose'].unique()


df['purpose'].value_counts()


df.groupby('purpose')['class'].value_counts(normalize=True)


df.groupby('checking_status')['class'].value_counts(normalize=True)



# Label encoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for i in cat:
    df[i] = le.fit_transform(df[i])

df.head()


df.to_csv('process.csv')


df['purpose'].value_counts()


plt.figure(figsize=(10,10))
sns.heatmap(np.round(df.corr(), 2),linewidths=0.1,vmax=1.0, 
            square=True,  linecolor='white', annot=True)
plt.show()


# Select features and target variable
x = df[df.columns.difference(['class'])]
y = df['class']


# Select the top 10 most important features using chi-squared
from sklearn.feature_selection import SelectKBest, chi2
selector = SelectKBest(chi2, k=20).fit(x, y)
kbest = pd.DataFrame(selector.transform(x), 
                 columns=selector.get_feature_names_out()) 

# Display the resulting feature set
kbest.head()


# initial modeling
from sklearn.preprocessing import StandardScaler
scaler =  StandardScaler()
Xscale  = scaler.fit_transform(x)
Xscale
#test_scaled = scaler.transform(X_test)


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(Xscale, y, test_size = 0.20, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape


y_train


# !pip install --pre pycaret

# from pycaret.classification import *

# s=setup(x,target=y)

# best=compare_models()

# print(best)

# finalize_model(best)

# evaluate_model(best)

# predict_model(best)


# !pip install catboost
# !pip install scikit-optimize
# !pip install lightgbm


from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \
                            precision_score, recall_score, f1_score, roc_auc_score, \
                            roc_curve,confusion_matrix
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from lightgbm import LGBMClassifier

from sklearn.model_selection import GridSearchCV





lr = LinearRegression().fit(X_train,y_train)
y_pred=lr.predict(X_test)
print("LinearRegression:")
print(f'training accuracy:{lr.score(X_train,y_train)}')
print(f'Testing accuracy:{lr.score(X_test,y_test)}')



logr=LogisticRegression().fit(X_train,y_train)
y_pred1=logr.predict(X_test)
cm1=confusion_matrix(y_test,y_pred1)
cr1 = classification_report(y_test,y_pred1)
print("LogisticRegression:\n")
print(f'training accuracy:{logr.score(X_train,y_train)}')
print(f'Testing accuracy:{logr.score(X_test,y_test)}')
print(f'confusion matrix:\n{cm1}')
print(f'classification report:\n{cr1}')


def evaluate_clf(true, predicted):
    acc = accuracy_score(true, predicted) # Calculate Accuracy
    f1 = f1_score(true, predicted) # Calculate F1-score
    precision = precision_score(true, predicted) # Calculate Precision
    recall = recall_score(true, predicted)  # Calculate Recall
    roc_auc = roc_auc_score(true, predicted) #Calculate Roc
    return acc, f1 , precision, recall, roc_auc


models = {
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "K-Neighbors Classifier": KNeighborsClassifier(),
    "XGBClassifier": XGBClassifier(), 
    "CatBoosting Classifier": CatBoostClassifier(verbose=False),
    "Support Vector Classifier": SVC(gamma='auto'),
    "AdaBoost Classifier": AdaBoostClassifier(),
    "Lightgbm Classifier": LGBMClassifier()

}


# Create a function which can evaluate models and return a report 
def evaluate_models(X, y, models):
    '''
    This function takes in X and y and models dictionary as input
    It splits the data into Train Test split
    Iterates through the given model dictionary and evaluates the metrics
    Returns: Dataframe which contains report of all models metrics with cost
    '''
    # separate dataset into train and test
    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
    
    models_list = []
    accuracy_list = []
    auc= []
    
    for i in range(len(list(models))):
        model = list(models.values())[i]
        model.fit(X_train, y_train) # Train model

        # Make predictions
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Training set performance
        model_train_accuracy, model_train_f1,model_train_precision,\
        model_train_recall,model_train_rocauc_score=evaluate_clf(y_train ,y_train_pred)


        # Test set performance
        model_test_accuracy,model_test_f1,model_test_precision,\
        model_test_recall,model_test_rocauc_score=evaluate_clf(y_test, y_test_pred)

        print(list(models.keys())[i])
        models_list.append(list(models.keys())[i])

        print('Model performance for Training set')
        print("- Accuracy: {:.4f}".format(model_train_accuracy))
        print('- F1 score: {:.4f}'.format(model_train_f1)) 
        print('- Precision: {:.4f}'.format(model_train_precision))
        print('- Recall: {:.4f}'.format(model_train_recall))
        print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))

        print('----------------------------------')

        print('Model performance for Test set')
        print('- Accuracy: {:.4f}'.format(model_test_accuracy))
        accuracy_list.append(model_test_accuracy)
        print('- F1 score: {:.4f}'.format(model_test_f1))
        print('- Precision: {:.4f}'.format(model_test_precision))
        print('- Recall: {:.4f}'.format(model_test_recall))
        print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))
        auc.append(model_test_rocauc_score)
        print('='*35)
        print('\n')
        
    report=pd.DataFrame(list(zip(models_list, accuracy_list)), columns=['Model Name', 'Accuracy']).sort_values(by=['Accuracy'], ascending=False)
        
    return report


# Pass raw scale data
model_report =evaluate_models(X=Xscale, y=y, models=models)


# Result models
model_report


#Initialize few parameter for Hyperparamter tuning
xgboost_params = {
    'max_depth': range(2, 10, 2),
    'min_child_weight':range (1, 10, 2)

}
lgbm_params={
    'n_estimators': range(1,10,100),
    'learning_rate': [0.5,1],
    'max_bin': [255]
}


# Models list for Hyperparameter tuning
randomcv_models = [
    ('XGBoost', XGBClassifier(), xgboost_params),
    ("LGBM", LGBMClassifier(), lgbm_params)
]


# Hypertunning Random_search and Grid_Search
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import KFold

kf = KFold(n_splits=30)

model_param = {}
for name, model, params in randomcv_models:
    print(name, model, params)
    random = RandomizedSearchCV(estimator=model,
                                    param_distributions=params,
                                   n_iter=100,
                                   cv=kf,
                                   verbose=2, 
                                   n_jobs=-1)
    random.fit(X_train, y_train)
    model_param[name] = random.best_params_

for model_name in model_param:
    print(f"---------------- Best Params for {model_name} -------------------")
    print(model_param[model_name])


model_param


# Testing with different dataset
from sklearn.metrics import roc_auc_score,roc_curve
best_models = {
    "XGBClassifier": XGBClassifier(**model_param['XGBoost'],n_jobs=-1),
    "LGBMClassifier": LGBMClassifier(**model_param['LGBM'])
}
tuned_report =evaluate_models(X=Xscale, y=y, models=best_models)


tuned_report


params = {'min_child_weight': 4, 'max_depth': 6}

best_model = XGBClassifier(**params)
# best_model = XGBClassifier(**model_param['XGBoost'])
best_model = best_model.fit(X_train,y_train)
y_pred_best = best_model.predict(X_test)
test_score = accuracy_score(y_test,y_pred_best)
cr = classification_report(y_test,y_pred_best)

print("FINAL MODEL 'XGBoost'")
print(f'Training Accuracy: {best_model.score(X_train,y_train)}')
print("Testing Accuracy Score: {:.4f}".format(test_score))
print (cr)



# ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)





from matplotlib.pyplot import figure
feat_importances = pd.Series(best_model.feature_importances_, index = x.columns).sort_values(ascending = True)
feat_importances.plot(kind = 'barh')


# Save best xgboost model
import joblib
import os
best_xgb_model = best_model

print(f"Best model: {best_xgb_model}")
joblib.dump(best_xgb_model, 'xgb_model_pipeline.pkl')





model = open('xgb_model_pipeline.pkl','rb')
xgb = joblib.load(model)


x


y.iloc[950]


trys=x.iloc[950].values#.reshape(1,-1)
trys


data=[[21,   0, 630,   1,  13,   4,   2,   1,   1,   2,   2,   1,   2,
         1,   1,   2,   0,   0,   2,   2]]


test1=scaler.transform(data)
test1


xgb.predict(test1)[0]



